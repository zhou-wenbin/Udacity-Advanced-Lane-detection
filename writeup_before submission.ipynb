{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Lane Detection** \n",
    "\n",
    "### In this project, I will demonstrade an advanced lane detection algorithm compared to previous one. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./camera_cal/calibration2.jpg \"distorted\"\n",
    "[image2]: ./output_images/undistorted_of2.png \"undistorted\"\n",
    "[image3]: ./test_images/test5.jpg \"test5\"\n",
    "[image4]: ./output_images/undistorted_of_test5.png \"undistorted of test5\"\n",
    "[image5]: ./test_images/straight_lines2.jpg \n",
    "[image6]: ./output_images/processed_of_test5.jpg\n",
    "[image7]: ./output_images/processed_of_straight_lines2.jpg\n",
    "[image8]: ./output_images/S_channel_of_test5.jpg\n",
    "[image9]: ./output_images/H_channel_of_test5.jpg\n",
    "[image10]: ./output_images/L_channel_of_test5.jpg\n",
    "[image11]: ./output_images/processed_of_test5.png\n",
    "[image12]: ./output_images/masked_of_test5.png\n",
    "[image13]: ./output_images/warped_of_test5.png\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Here I will consider the [rubric](https://review.udacity.com/#!/rubrics/571/view) points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1.Camera calibration\n",
    "\n",
    "#### 1.1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"./camera calibrating.ipynb\"ã€‚\n",
    "\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then use the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image in \"./camera_cal/calibration2.jpg\" using the `cv2.undistort()` function. For later usage, I will save camera calibration and distortion coefficients in a file located in \"./camera_cal/wide_dist_pickle.p\".\n",
    "* The distorted and undistorted testing images are shown as follows:\n",
    "\n",
    "Distorted             |  Undistorted\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt text][image1]  |  ![alt text][image2]\n",
    "\n",
    "* Let us try to distort a road image taken by the camera:\n",
    "\n",
    "Distorted             |  Undistorted\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt text][image3]  |  ![alt text][image4]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Image processing with color transform and gradient thresholds\n",
    "\n",
    "The code for this step is contained in the IPython notebook located in \"TBA\"\n",
    "\n",
    "#### 2.1 Color channel choosing\n",
    "\n",
    "In the task of lane detection when the shodow of road trees are projected onto the road, it becomes a bit more difficult to detect the lane lines that are covered in the shadow. However, when we do the color transfor of the image, we will overcome this issue. In what follows, I will explain in details.\n",
    "\n",
    "##### 2.1.1 The bottleneck to detect lane lines by only using gradiant of an image\n",
    "\n",
    "Normally, it is possible to detect lane lines from an image by shresholding the gradient in the x direction since we have the common sense that the lane lines are almost perpendicular to the y direction of an image. In the following two unprocessed images if we use same shreshold of x gradient to detect lane lines we will see that the image1 with shadow performs worse. In particular, those lines covered in the shadow cannot be detected.\n",
    "\n",
    "\n",
    "Image1             |  Image2\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt text][image3]  |  ![alt text][image5]\n",
    "\n",
    "\n",
    "Processed Image1             |  Processed Image2\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt text][image6]  |  ![alt text][image7]\n",
    "\n",
    "##### 2.1.2 Color transform\n",
    "In what follows, I will transform images to HLS color space (hue, lightness, and saturation), which is one of the most commonly used color spaces in image analysis. To get some intuition about these color spaces, you can generally think of Hue as the value that represents color independent of any change in brightness. So if you imagine a basic red paint color, then add some white to it or some black to make that color lighter or darker -- the underlying color remains the same and the hue for all of these colors will be the same.\n",
    "\n",
    "On the other hand, Lightness and Value represent different ways to measure the relative lightness or darkness of a color. For example, a dark red will have a similar hue but much lower value for lightness than a light red. Saturation also plays a part in this; saturation is a measurement of colorfulness. So, as colors get lighter and closer to white, they have a lower saturation value, whereas colors that are the most intense, like a bright primary color (imagine a bright red, blue, or yellow), have a high saturation value. You can get a better idea of these values by looking at the 3D color spaces pictured below.\n",
    "\n",
    "Next, let's us apply this theory to a testing image. First, let's take a look at the three channels of an HIS space image. \n",
    "\n",
    "\n",
    "S channle of Image             |  H channel of  Image  |  L channel of  Image\n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "![alt text][image8]  |  ![alt text][image9] |![alt text][image10] \n",
    "\n",
    "\n",
    "From the above, we notice that S channel highlights the yellow line very well, so we will use the channel to do the shreshold manipulation. \n",
    "\n",
    "#### 2.2 Combine color channel and x gradient to shreshold the image. \n",
    "\n",
    "We will combine the shreholds discussed above to process image to find lane lines (typically the yellow lines). The following two images are the contrast. We set the shresholds as follows by test and trials:\n",
    "\n",
    "| gradient shreshold        |    S channel shreshold   | \n",
    "|:-------------:|:-------------:| \n",
    "| 20,100     | 170,255       | \n",
    "\n",
    "We show the images as follows after the thresholds, since it looks well in detecting the yellow line covered in the shadow, we will use the combined shresholds to process images.\n",
    "\n",
    "Original             |  Processed\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt text][image3]  |  ![alt text][image11]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3. Get birdseye view of warped lines in an image\n",
    "\n",
    "In order to draw lines on the warped lane lines on the road, we could firstly find those warped lines through birdseye view of an image.  This can be done by selecting the specific area where the warped lines. Note that the source area and destination area were both tested in the notebook.\n",
    "\n",
    "Original             |  Warped\n",
    ":-------------------------:|:-------------------------:\n",
    "![alt text][image12]  |  ![alt text][image13]\n",
    "\n",
    "### 4.  Identified lane-line pixels and fit their positions with a polynomial in the warped image\n",
    "\n",
    "<!-- ### Pipeline (single images)\n",
    "\n",
    "#### 1. Provide an example of a distortion-corrected image.\n",
    "\n",
    "To demonstrate this step, I will describe how I apply the distortion correction to one of the test images like this one:\n",
    "![alt text][image2]\n",
    "\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.\n",
    "\n",
    "I used a combination of color and gradient thresholds to generate a binary image (thresholding steps at lines # through # in `another_file.py`).  Here's an example of my output for this step.  (note: this is not actually from one of the test images)\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "The code for my perspective transform includes a function called `warper()`, which appears in lines 1 through 8 in the file `example.py` (output_images/examples/example.py) (or, for example, in the 3rd code cell of the IPython notebook).  The `warper()` function takes as inputs an image (`img`), as well as source (`src`) and destination (`dst`) points.  I chose the hardcode the source and destination points in the following manner:\n",
    "\n",
    "```python\n",
    "src = np.float32(\n",
    "    [[(img_size[0] / 2) - 55, img_size[1] / 2 + 100],\n",
    "    [((img_size[0] / 6) - 10), img_size[1]],\n",
    "    [(img_size[0] * 5 / 6) + 60, img_size[1]],\n",
    "    [(img_size[0] / 2 + 55), img_size[1] / 2 + 100]])\n",
    "dst = np.float32(\n",
    "    [[(img_size[0] / 4), 0],\n",
    "    [(img_size[0] / 4), img_size[1]],\n",
    "    [(img_size[0] * 3 / 4), img_size[1]],\n",
    "    [(img_size[0] * 3 / 4), 0]])\n",
    "```\n",
    "\n",
    "This resulted in the following source and destination points:\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 585, 460      | 320, 0        | \n",
    "| 203, 720      | 320, 720      |\n",
    "| 1127, 720     | 960, 720      |\n",
    "| 695, 460      | 960, 0        |\n",
    "\n",
    "I verified that my perspective transform was working as expected by drawing the `src` and `dst` points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image.\n",
    "\n",
    "![alt text][image4]\n",
    "\n",
    "#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "Then I did some other stuff and fit my lane lines with a 2nd order polynomial kinda like this:\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "I did this in lines # through # in my code in `my_other_file.py`\n",
    "\n",
    "#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "I implemented this step in lines # through # in my code in `yet_another_file.py` in the function `map_lane()`.  Here is an example of my result on a test image:\n",
    "\n",
    "![alt text][image6]\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline (video)\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n",
    "\n",
    "Here's a [link to my video result](./project_video.mp4)\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
